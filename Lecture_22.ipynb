{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 22\n",
    "\n",
    "## Transformations, LogNormal, Convolutions, Proving Existence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance of Hypergeometric, con't\n",
    "\n",
    "Returning to where we left off in Lecture 21, recall that we are considiering $X \\sim HGeom(w, b, n)$ where $p = \\frac{w}{w+b}$ and $w + b = N$.\n",
    "\n",
    "\\begin{align}\n",
    "  Var\\left( \\sum_{j=1}^{n} Var(X_j) \\right) &= Var(X_1) + \\dots + Var(X_n) + 2 \\, \\sum_{i<j} Cov(X_i, X_j) \\\\\n",
    "  &= n \\, Var(X_1) + 2 \\, \\binom{n}{2} Cov (X_1, X_2) & \\quad \\text{symmetry, amirite?} \\\\\n",
    "  &= n \\, p \\, (1-p) + 2 \\, \\binom{n}{2} \\left( \\frac{w}{w+b} \\, \\frac{w-1}{w+b-1} - p^2  \\right) \\\\\n",
    "  &= \\frac{N-n}{N-1} \\, n \\, p \\, (1-p) \\\\\n",
    "  \\\\\n",
    "  \\text{where } \\frac{N-n}{N-1} &\\text{  is known as the finite population correction}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note how this closely resembles the variance for a binomial distribution, except for scaling by that finite population correction.\n",
    "\n",
    "Let's idiot-check this:\n",
    "\n",
    "\\begin{align}\n",
    "  \\text{let } n &= 1 \\\\\n",
    "  \\\\\n",
    "  Var(X) &= \\frac{N-1}{N-1} 1 \\, p \\, (1-p) \\\\\n",
    "  &= p \\, (1-p) & \\quad \\text{ ... just a Bernoulli, since we only sample once!} \\\\\n",
    "  \\\\\n",
    "  \\text{let } N &\\gg n \\\\\n",
    "  \\Rightarrow \\frac{N-n}{N-1} &= 1\n",
    "  \\\\\n",
    "  Var(X) &= \\frac{N-n}{N-1} n \\, p \\, (1-p) \\\\\n",
    "  &= n \\, p \\, (1-p) & \\quad \\text{ ... Binomial, we probably never sample same element twice!} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "### Or a change of variables\n",
    "\n",
    "A function of an r.v. is itself an r.v., and we can use LOTUS to find mean and variance. But what if we want more than just the mean and variance? What if we want to know the entire distribution?\n",
    "\n",
    "### Theorem\n",
    "\n",
    "> Let $X$ be a continuous r.v. with PDF $f_X, Y = g(X)$.\n",
    "> Given that $g$ is differentiable, and strictly increasing,\n",
    "> then the PDF of $Y$ is given by\n",
    ">\n",
    "> \\begin\\{align\\}\n",
    ">   f_Y(y) &= f_X(x) \\, \\frac{dx}{dy} & \\quad \\text{ where } y = g(x) \\text{ , } x = g^{-1}(y) \\\\\n",
    "> \\end\\{align\\}\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
