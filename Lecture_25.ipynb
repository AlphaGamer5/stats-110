{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 25\n",
    "# Beta-Gamma (bank-post office), order statistics, conditional expectation, two envelope paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the Gamma and Beta Distributions\n",
    "\n",
    "Say you have to visit both the bank and the post office today. What can we say about the total times you have to wait in the lines?\n",
    "\n",
    "Let $X \\sim Gamma(a, \\lambda)$ be the total time you wait in line at the bank, given that there are $a$ people in line in front of you, and the waiting times are i.i.d $Expo(\\lambda)$; recall the analogies of geometric $\\rightarrow$ negative binomial, and of exponential $\\rightarrow$ gamma. The waiting time in line at the bank for everyone individually is $Expo(\\lambda)$, and as the $a+1^{th}$ person, your time in line is sum of those $a$ $Expo(\\lambda)$ times.\n",
    "\n",
    "Similarly, let $Y \\sim Gamma(b, \\lambda)$ be the total time you wait in line at the post office, given that there are $b$ people in line in front of you.\n",
    "\n",
    "Assume that $X, Y$ are independent.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. What is the distribution of $T = X + Y$?\n",
    "1. Given $T = X + Y$ and $W = \\frac{X}{X+Y}$, what is the joint distribution?\n",
    "1. Are $T, W$ independent?\n",
    "\n",
    "### What is the distribution of $T$?\n",
    "\n",
    "We immediately know that the total time you spend waiting in the lines is\n",
    "\n",
    "\\begin{align}\n",
    "  T &= X + Y \\\\\n",
    "    &\\sim Gamma(a+b, \\lambda)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### What is the distribution of  $T,W$?\n",
    "\n",
    "Let $\\lambda = 1$, to make the calculation simpler. We do not lose any generality, since we can scale by $\\lambda$ later.\n",
    "\n",
    "So we are looking the joint PDF of $T,W$\n",
    "\n",
    "\\begin{align}\n",
    "  \\text{joint PDF } f_{T,W}(t,w) &= f_{X,Y}(x,y) \\, \\left| \\frac{\\partial(x,y)}{\\partial(t,w)} \\right| \\\\\n",
    "  &= \\frac{1}{\\Gamma(a) \\Gamma(b)} \\, x^a \\, e^{-x} \\, y^b \\, e^{-y} \\, \\frac{1}{xy} \\, \\left| \\frac{\\partial(x,y)}{\\partial(t,w)} \\right| \\\\\\\\\n",
    "  \\\\\n",
    "  \\text{for the Jacobian, let } x + y &= t \\\\\n",
    "  \\frac{x}{x+y} &= w \\\\\n",
    "  \\\\\n",
    "  \\Rightarrow x &= tw \\\\\n",
    "  \\\\\n",
    "  1 - \\frac{x}{x+y} &= 1 - w \\\\\n",
    "  \\frac{x + y - x}{t} &= 1 - w \\\\\n",
    "  \\\\\n",
    "  \\Rightarrow y &= t(1-w) \\\\\\\\\n",
    "  \\\\\n",
    "  \\left| \\frac{\\partial(x,y)}{\\partial(t,w)} \\right| &= \n",
    "    \\begin{bmatrix}\n",
    "      \\frac{\\partial x}{\\partial t} & \\frac{\\partial x}{\\partial w} \\\\\n",
    "      \\frac{\\partial y}{\\partial t} & \\frac{\\partial y}{\\partial w} \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "      w & t \\\\\n",
    "      1-w & -t \n",
    "    \\end{bmatrix} \\\\\n",
    "    &= -tw - t(1-w) \\\\\n",
    "    &= -t \\\\\\\\\n",
    "    \\\\\n",
    "    \\text{returning to PDF } f_{T,W}(t,w) &=  \\frac{1}{\\Gamma(a) \\Gamma(b)} \\, x^a \\, e^{-x} \\, y^b \\, e^{-y} \\, \\frac{1}{xy} \\, \\left| \\frac{\\partial(x,y)}{\\partial(t,w)} \\right| \\\\\n",
    "    &= \\frac{1}{\\Gamma(a) \\Gamma(b)} \\, (tw)^a \\, e^{-(tw)} \\, (t(1-w))^b \\, e^{-t(1-w)} \\, \\frac{1}{tw \\, t(1-w)} \\, t \\\\\n",
    "    &= \\frac{1}{\\Gamma(a) \\Gamma(b)} \\, w^{a-1} \\, (1-w)^{b-1} \\,\\, t^{a+b} \\, e^{-t} \\, \\frac{1}{t} \\, c &\\quad \\text{ where } c \\text{ is the normalizing constant for } T \\\\\n",
    "    &= \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\, w^{a-1} \\, (1-w)^{b-1} \\,\\, \\frac{t^{a+b} \\, e^{-t} \\, \\frac{1}{t}}{\\Gamma(a+b)} &\\quad \\text{ multiplying by } 1 \n",
    "\\end{align}\n",
    "\n",
    "Since we are able to successfully derive $f_{T,W}(t,w)$ in terms of $T \\sim Gamma(a,b)$ and $W \\sim Beta(a,b)$, this means we have also answered the third question: _$T,W$ are independent_.\n",
    "\n",
    "### Unexpected Discovery: Normalizing Constant for Beta\n",
    "\n",
    "Now say we are interested in finding the marginal PDF for $W$\n",
    "\n",
    "\\begin{align}\n",
    "  f_{W}(w) &= \\int_{-\\infty}^{\\infty} f_{T,W}(t,w) dt \\\\\n",
    "  &= \\int_{-\\infty}^{\\infty} \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\, w^{a-1} \\, (1-w)^{b-1} \\,\\, \\frac{t^{a+b} \\, e^{-t} \\, \\frac{1}{t}}{\\Gamma(a+b)} \\, dt \\\\\n",
    "  &= \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\, w^{a-1} \\, (1-w)^{b-1} \\, \\int_{-\\infty}^{\\infty} \\frac{t^{a+b} \\, e^{-t} \\, \\frac{1}{t}}{\\Gamma(a+b)} \\, dt\\\\\n",
    "  &= \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\, w^{a-1} \\, (1-w)^{b-1} \n",
    "\\end{align}\n",
    "\n",
    "But notice that since marginal PDF $f_{W}(w)$ must integrate to 1, then $\\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)}$ is the normalizing constant for the Beta distribution! If this were not true, then $f_{W}(w)$ could not be a valid PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example Usage: Finding $\\mathbb{E}(W), W \\sim Beta(a,b)$\n",
    "\n",
    "There are two ways you could find $\\mathbb{E}(W)$.\n",
    "\n",
    "You could use LOTUS, where you would simply do:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbb{E}(W) &= \\int \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\, w^{a-1} \\, (1-w)^{b-1} \\, w \\, dw \\\\\n",
    "  &= \\int \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\, w^{a} \\, (1-w)^{b-1} \\, dw \\\\\n",
    "\\end{align}\n",
    "\n",
    "... and would not be so hard to handle, since that also is a $Beta$.\n",
    "\n",
    "Or, since we are continuing on the topic of $W = X + Y$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbb{E}(W) &= \\mathbb{E}\\left( \\frac{X}{X+Y} \\right) \\\\\n",
    "  &= \\frac{\\mathbb{E}(X)}{\\mathbb{E}(X+Y)} \\quad \\text{ which is true, under certain conditions}\n",
    "\\end{align}\n",
    "\n",
    "So why is $\\mathbb{E}\\left( \\frac{X}{X+Y} \\right) = \\frac{\\mathbb{E}(X)}{\\mathbb{E}(X+Y)}$?\n",
    "\n",
    "Facts\n",
    "\n",
    "1. since $T$ is independent of $W$, $\\frac{X}{X+Y}$ is independent of $X+Y$\n",
    "2. since independence implies they are uncorrelated, $\\frac{X}{X+Y}$ and $X+Y$ are therefore _uncorrelated_ \n",
    "3. by definition of uncorreleted, \\begin{align}\n",
    "  \\mathbb{E}(AB) - \\mathbb{E}(A) \\, \\mathbb{E}(B) &= 0 \\\\\n",
    "  \\mathbb{E}(AB) &= \\mathbb{E}(A) \\, \\mathbb{E}(B) \\\\\\\\\n",
    "  \\\\\n",
    "  \\mathbb{E} \\left( \\frac{X}{X+Y} \\, (X+Y) \\right) &= \\mathbb{E}(\\frac{X}{X+Y}) \\, \\mathbb{E}(X+Y) \\\\\n",
    "  \\mathbb{E}(X) &= \\mathbb{E}(\\frac{X}{X+Y}) \\, \\mathbb{E}(X+Y) \\\\\n",
    "  \\Rightarrow \\mathbb{E}\\left( \\frac{X}{X+Y} \\right) &= \\frac{\\mathbb{E}(X)}{\\mathbb{E}(X+Y)} \\\\\\\\\n",
    "  \\\\\n",
    "  \\therefore \\mathbb{E}(W) &= \\mathbb{E} \\left( \\frac{X}{X+Y} \\right) \\\\\n",
    "  &= \\frac{\\mathbb{E}(X)}{\\mathbb{E}(X+Y)} \\\\\n",
    "  &= \\frac{a}{a+b}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
